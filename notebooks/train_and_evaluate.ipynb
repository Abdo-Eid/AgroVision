{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AgroVision Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de30db12",
      "metadata": {},
      "source": [
        "# About this notebook\n",
        "This notebook guides you through training and evaluating the AgroVision semantic segmentation model using the `agrovision_core` library. It is organized into small, focused sections:\n",
        "\n",
        "1. Setup: ensure we run from the repository root so paths resolve correctly. ✅\n",
        "2. Imports: bring in the functions used for training and evaluation.\n",
        "3. Run training & evaluation: runs model training (can be long; see guidance below).\n",
        "4. Data inspection: quick checks on the processed mask files and class distribution.\n",
        "5. Visualization: visualize a single validation tile and the model prediction.\n",
        "\n",
        "Quick tips:\n",
        "- If you want to run a fast smoke test, set `cfg['training']['epochs'] = 1` and `training_cfg['num_workers'] = 0` before calling `train(cfg)`.\n",
        "- Running full training may take many minutes/hours depending on hardware; use the run-instructions cell to customize.\n",
        "\n",
        "Make sure you are using the project's Python environment (e.g. via `uv run jupyter lab` or the project's virtual env)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "73bc53c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory is now: d:\\trying\\AgroVision\n"
          ]
        }
      ],
      "source": [
        "# Set the working directory to the repository root so relative paths (config, data, outputs) resolve reliably.\n",
        "# This is useful when running the notebook from inside the `notebooks/` folder in VS Code.\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd().parent\n",
        "os.chdir(ROOT)\n",
        "print(\"Working directory is now:\", Path.cwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b7ddaec3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick explanation of imports below: these bring training, evaluation, and config helpers from the library.\n",
        "# - `train` performs model training and returns (model, metrics)\n",
        "# - `evaluate` runs evaluation and returns a metrics dict\n",
        "# - `load_config` reads the YAML configuration used to control training/evaluation\n",
        "from agrovision_core.train.train import train\n",
        "from agrovision_core.train.evaluate import evaluate\n",
        "from agrovision_core.utils.io import load_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "251482f6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.05"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load configuration: this loads `config/config.yaml` into `cfg` (a Python dict)\n",
        "# You can inspect `cfg` to see dataset paths, class ids, training hyperparams, etc.\n",
        "cfg = load_config(\"config/config.yaml\")\n",
        "\n",
        "# Ensure training defaults are set and safe for quick debugging\n",
        "training_cfg = cfg.setdefault(\"training\", {})\n",
        "training_cfg.setdefault(\"ignore_index\", 0)\n",
        "training_cfg.setdefault(\"min_labeled_fraction\", 0.05)\n",
        "\n",
        "# Optional: avoid Windows/Jupyter multiprocessing issues by using a single worker\n",
        "# training_cfg[\"num_workers\"] = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cd1bfc15",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using class weights (normalized mean=1): [0.0, 0.017464593052864075, 0.02802138775587082, 0.45504868030548096, 0.036044325679540634, 2.4706315994262695, 0.22541329264640808, 0.41647788882255554, 0.14953896403312683, 0.3745090961456299, 1.9349637031555176, 1.4807058572769165, 5.0264573097229, 0.3847229480743408]\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 22] Invalid argument",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# WARNING: The next line will run model training, which can be long depending on your machine.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# For a quick smoke test, set `cfg['training']['epochs'] = 1` before calling train(cfg).\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model, train_metrics = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Run evaluation on the trained model (optional) — this returns a dict of metrics.\u001b[39;00m\n\u001b[32m      5\u001b[39m eval_results = evaluate(model, cfg)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mD:\\trying\\AgroVision\\agrovision_core\\src\\agrovision_core\\train\\train.py:375\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    372\u001b[39m model.train()\n\u001b[32m    373\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    376\u001b[39m     images = batch[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m].to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    377\u001b[39m     masks = batch[\u001b[33m\"\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m\"\u001b[39m].to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\trying\\AgroVision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\trying\\AgroVision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\trying\\AgroVision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\multiprocessing\\context.py:337\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     94\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     97\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument"
          ]
        }
      ],
      "source": [
        "# WARNING: The next line will run model training, which can be long depending on your machine.\n",
        "# For a quick smoke test, set `cfg['training']['epochs'] = 1` before calling train(cfg).\n",
        "model, train_metrics = train(cfg)\n",
        "# Run evaluation on the trained model (optional) — this returns a dict of metrics.\n",
        "eval_results = evaluate(model, cfg)\n",
        "train_metrics, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faddbbbd",
      "metadata": {},
      "source": [
        "# Guidance before running training\n",
        "# - Make any configuration changes now (e.g., reduce epochs for a faster run)\n",
        "# - If you get CUDA errors, set device to 'cpu' in cfg: cfg['model']['device'] = 'cpu'\n",
        "# - If you only want to evaluate an existing checkpoint, skip the `train` call and call `evaluate(model=None, cfg=cfg, checkpoint_path='path/to/checkpoint.pth')`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b8e6472",
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = load_config(\"config/config.yaml\")\n",
        "\n",
        "training_cfg = cfg.setdefault(\"training\", {})\n",
        "training_cfg.setdefault(\"ignore_index\", 0)\n",
        "training_cfg.setdefault(\"min_labeled_fraction\", 0.05)\n",
        "\n",
        "# Optional: avoid Windows/Jupyter multiprocessing issues\n",
        "# training_cfg[\"num_workers\"] = 0\n",
        "\n",
        "model, train_metrics = train(cfg)\n",
        "eval_results = evaluate(model, cfg)\n",
        "train_metrics, eval_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0e9500",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a validation dataset and load a small batch for spot-checking\n",
        "processed_dir = resolve_path(cfg[\"paths\"][\"data_processed\"])\n",
        "val_dataset = CropDataset(processed_dir, split=\"val\")\n",
        "contig_ids = sorted(val_dataset.index_to_raw.keys())\n",
        "if contig_ids != list(range(len(contig_ids))):\n",
        "    print(\"Warning: non-contiguous class_map values:\", contig_ids)\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "batch = next(iter(val_loader))\n",
        "images = batch[\"image\"].to(device)\n",
        "gt_masks = batch[\"mask\"].cpu().numpy()\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    logits = model(images)\n",
        "pred_masks = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "# Build color map from cfg (raw ids)\n",
        "color_map = {}\n",
        "for raw_id, info in cfg.get(\"classes\", {}).items():\n",
        "    try:\n",
        "        color_map[int(raw_id)] = info.get(\"color\", [0, 0, 0])\n",
        "    except (TypeError, ValueError):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26bf87e",
      "metadata": {},
      "source": [
        "# What this check does\n",
        "# - Confirms that the processed validation masks exist and can be opened\n",
        "# - Shows which raw class ids appear in the masks and checks against the `classes` config\n",
        "# - Loads a small validation batch and runs a forward pass to produce predictions\n",
        "# Use these outputs to validate data processing, label mappings, and quick model sanity checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3bfb47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick visualization of one validation tile\n",
        "# This cell is intended to show a single image, the ground truth mask, and the model's prediction.\n",
        "# Requirements: `cfg`, `model` and a prepared `val_dataset` or run the data-check cell above first.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from agrovision_core.data.dataset import CropDataset\n",
        "from agrovision_core.utils.io import resolve_path\n",
        "\n",
        "if \"cfg\" not in globals() or \"model\" not in globals():\n",
        "    raise RuntimeError(\"Run the config cell and training cell first.\")\n",
        "\n",
        "processed_dir = resolve_path(cfg[\"paths\"][\"data_processed\"])\n",
        "val_dataset = CropDataset(processed_dir, split=\"val\")\n",
        "device = next(model.parameters()).device\n",
        "sample = val_dataset[0]\n",
        "image = sample[\"image\"]\n",
        "mask = sample[\"mask\"].cpu().numpy()\n",
        "\n",
        "ignore_index = int(cfg.get(\"training\", {}).get(\"ignore_index\", 0))\n",
        "\n",
        "# Run the model on one image (no gradient needed)\n",
        "with torch.inference_mode():\n",
        "    pred_logits = model(image.unsqueeze(0).to(device))\n",
        "pred_mask = torch.argmax(pred_logits, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Build color map for raw ids from the config\n",
        "color_map = {}\n",
        "for raw_id, info in cfg.get(\"classes\", {}).items():\n",
        "    try:\n",
        "        color_map[int(raw_id)] = info.get(\"color\", [0, 0, 0])\n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        "\n",
        "index_to_raw = val_dataset.index_to_raw\n",
        "\n",
        "def _mask_to_rgb_quick(mask_contig):\n",
        "    h, w = mask_contig.shape\n",
        "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for contig_id, raw_id in index_to_raw.items():\n",
        "        rgb[mask_contig == contig_id] = color_map.get(int(raw_id), [0, 0, 0])\n",
        "    rgb[mask_contig == ignore_index] = [255, 255, 255]\n",
        "    return rgb\n",
        "\n",
        "band_names = [b.get(\"name\") for b in cfg.get(\"bands\", [])]\n",
        "band_idx = {name: i for i, name in enumerate(band_names) if name}\n",
        "rgb_indices = [band_idx[name] for name in (\"B04\", \"B03\", \"B02\") if name in band_idx]\n",
        "if len(rgb_indices) != 3:\n",
        "    rgb_indices = [0, 1, 2]\n",
        "\n",
        "def _to_rgb_quick(image_tensor):\n",
        "    arr = image_tensor.cpu().numpy()\n",
        "    rgb = arr[rgb_indices, :, :]\n",
        "    rgb = np.stack(rgb, axis=-1)\n",
        "    min_val = rgb.min(axis=(0, 1), keepdims=True)\n",
        "    max_val = rgb.max(axis=(0, 1), keepdims=True)\n",
        "    rgb = (rgb - min_val) / np.clip(max_val - min_val, 1e-6, None)\n",
        "    return rgb\n",
        "\n",
        "# Plot results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "axes[0].imshow(_to_rgb_quick(image))\n",
        "axes[0].set_title(\"Image\")\n",
        "axes[1].imshow(_mask_to_rgb_quick(mask))\n",
        "axes[1].set_title(\"GT\")\n",
        "axes[2].imshow(_mask_to_rgb_quick(pred_mask))\n",
        "axes[2].set_title(\"Prediction\")\n",
        "for ax in axes:\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
