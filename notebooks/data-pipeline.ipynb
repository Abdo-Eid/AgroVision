{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "j9fp4kqm6g8",
   "metadata": {},
   "source": [
    "# AgriFieldNet Data Pipeline\n",
    "\n",
    "This notebook downloads and prepares the **AgriFieldNet India Challenge** dataset for crop classification using TorchGeo.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Source** | Sentinel-2 satellite imagery |\n",
    "| **Resolution** | 10m (visible/NIR), 20m (red edge/SWIR) |\n",
    "| **Tile Size** | 256 × 256 pixels |\n",
    "| **Total Tiles** | 1,217 tiles |\n",
    "| **Training Fields** | 5,551 fields |\n",
    "| **Test Fields** | 1,530 fields |\n",
    "| **Crop Classes** | 13 classes |\n",
    "\n",
    "### Crop Classes\n",
    "\n",
    "| ID | Crop (English) | Crop (Arabic) |\n",
    "|----|----------------|---------------|\n",
    "| 1 | Wheat | قمح |\n",
    "| 2 | Mustard | خردل |\n",
    "| 3 | Lentil | عدس |\n",
    "| 4 | No Crop / Fallow | بور |\n",
    "| 5 | Green Pea | بسلة خضراء |\n",
    "| 6 | Sugarcane | قصب السكر |\n",
    "| 8 | Garlic | ثوم |\n",
    "| 9 | Maize | ذرة صفراء |\n",
    "| 13 | Gram / Chickpea | حمص |\n",
    "| 14 | Coriander | كزبرة |\n",
    "| 15 | Potato | بطاطس |\n",
    "| 16 | Berseem | برسيم |\n",
    "| 36 | Rice | أرز |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ajhxao5v7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We use **TorchGeo**, a PyTorch library specifically designed for geospatial deep learning.\n",
    "\n",
    "- **`AgriFieldNet`**: The dataset class that handles loading Sentinel-2 imagery and crop labels\n",
    "- **`AgriFieldNetDataModule`**: A PyTorch Lightning DataModule that wraps the dataset with DataLoaders, samplers, and train/val/test splits\n",
    "\n",
    "TorchGeo abstracts away the complexity of:\n",
    "- Coordinate reference systems (CRS)\n",
    "- Geospatial file formats (GeoTIFF)\n",
    "- Multi-resolution band alignment\n",
    "- Spatial sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38ad56",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.12)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"f:/collage/AI 4/term 1/CNN for Visual Recognition/AgroVision/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchgeo.datasets import AgriFieldNet\n",
    "from torchgeo.datamodules import AgriFieldNetDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vgr07e0s2j",
   "metadata": {},
   "source": [
    "## 2. Setup Data Directory\n",
    "\n",
    "Create the local directory where the dataset will be stored. The dataset requires approximately **2-3 GB** of disk space.\n",
    "\n",
    "```\n",
    "data/\n",
    "└── agrifieldnet/\n",
    "    ├── source/           # Sentinel-2 bands (B01-B12)\n",
    "    ├── train_labels/     # Crop type labels + field IDs\n",
    "    └── test_labels/      # Test set field IDs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dxlxsb5ta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data/agrifieldnet\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v673odrgm3d",
   "metadata": {},
   "source": [
    "## 3. Download the Dataset\n",
    "\n",
    "This cell downloads the AgriFieldNet dataset from **Source Cooperative** using `azcopy`.\n",
    "\n",
    "### What gets downloaded:\n",
    "\n",
    "| Component | Description | Size |\n",
    "|-----------|-------------|------|\n",
    "| **Source imagery** | 12 Sentinel-2 spectral bands per tile | ~2 GB |\n",
    "| **Train labels** | `raster_labels.tif` (crop classes) + `field_ids.tif` | ~500 MB |\n",
    "| **Test labels** | `field_ids.tif` only (no crop labels) | ~200 MB |\n",
    "\n",
    "### Sentinel-2 Bands Included:\n",
    "\n",
    "| Band | Name | Wavelength | Resolution | Purpose |\n",
    "|------|------|------------|------------|---------|\n",
    "| B01 | Coastal Aerosol | 443 nm | 60m | Atmospheric correction |\n",
    "| B02 | Blue | 490 nm | 10m | Water, vegetation |\n",
    "| B03 | Green | 560 nm | 10m | Vegetation health |\n",
    "| B04 | Red | 665 nm | 10m | Chlorophyll absorption |\n",
    "| B05 | Red Edge 1 | 705 nm | 20m | Vegetation stress |\n",
    "| B06 | Red Edge 2 | 740 nm | 20m | Leaf area index |\n",
    "| B07 | Red Edge 3 | 783 nm | 20m | Canopy structure |\n",
    "| B08 | NIR | 842 nm | 10m | Biomass (NDVI) |\n",
    "| B8A | NIR Narrow | 865 nm | 20m | Water vapor |\n",
    "| B09 | Water Vapor | 945 nm | 60m | Atmospheric |\n",
    "| B11 | SWIR 1 | 1610 nm | 20m | Moisture content |\n",
    "| B12 | SWIR 2 | 2190 nm | 20m | Mineral/dry vegetation |\n",
    "\n",
    "> **Note**: Download time varies from 10-30 minutes depending on internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf5nu2sojkl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset (this will use azcopy)\n",
    "# This may take a while depending on your internet connection\n",
    "\n",
    "dataset = AgriFieldNet(\n",
    "    paths=\"data/agrifieldnet\",\n",
    "    download=True,  # This triggers the download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gxrzm89qvn7",
   "metadata": {},
   "source": [
    "## 4. Verify the Dataset\n",
    "\n",
    "After download, we verify the dataset loaded correctly by checking:\n",
    "\n",
    "- **CRS (Coordinate Reference System)**: The geographic projection used (typically EPSG:32643 - UTM Zone 43N for India)\n",
    "- **Bands**: List of available spectral bands\n",
    "- **Files**: Number of GeoTIFF files indexed by TorchGeo\n",
    "\n",
    "### Expected Output:\n",
    "```\n",
    "Dataset CRS: EPSG:32643\n",
    "Dataset bands: ('B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12')\n",
    "Number of files: 1217\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v3cdmhi9v1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the dataset\n",
    "print(f\"Dataset CRS: {dataset.crs}\")\n",
    "print(f\"Dataset bands: {dataset.bands}\")\n",
    "print(f\"Number of files: {len(dataset.files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xgbxsgj1hw",
   "metadata": {},
   "source": [
    "## 5. Setup PyTorch Lightning DataModule\n",
    "\n",
    "The `AgriFieldNetDataModule` provides a complete data pipeline for training:\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `batch_size` | 32 | Number of samples per training batch |\n",
    "| `patch_size` | 256 | Size of each image patch (256×256 pixels) |\n",
    "| `num_workers` | 4 | Parallel data loading threads |\n",
    "| `paths` | \"data/agrifieldnet\" | Location of downloaded data |\n",
    "\n",
    "### What `setup(\"fit\")` does:\n",
    "\n",
    "1. **Indexes** all GeoTIFF files in the dataset\n",
    "2. **Creates spatial index** for efficient sampling\n",
    "3. **Splits** data into training and validation sets\n",
    "4. **Prepares** RandomGeoSampler for training batches\n",
    "\n",
    "### DataModule Methods:\n",
    "\n",
    "```python\n",
    "datamodule.train_dataloader()  # Returns training DataLoader\n",
    "datamodule.val_dataloader()    # Returns validation DataLoader\n",
    "datamodule.test_dataloader()   # Returns test DataLoader\n",
    "```\n",
    "\n",
    "### Batch Structure:\n",
    "\n",
    "Each batch from the DataLoader contains a dictionary:\n",
    "```python\n",
    "{\n",
    "    \"image\": Tensor[B, C, H, W],  # B=batch, C=12 bands, H=W=256\n",
    "    \"mask\": Tensor[B, H, W],      # Crop class labels (0-36)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lfr4xhqwwpk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use the DataModule\n",
    "datamodule = AgriFieldNetDataModule(\n",
    "    batch_size=32,\n",
    "    patch_size=256,\n",
    "    num_workers=4,\n",
    "    paths=\"data/agrifieldnet\",\n",
    ")\n",
    "\n",
    "# Prepare data (sets up train/val/test splits)\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ob27ff2zydh",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Custom Data Preprocessing Pipeline\n",
    "\n",
    "The TorchGeo DataModule above is useful for quick experimentation, but for production training we need:\n",
    "1. **Faster loading** - Pre-processed `.npy` files load much faster than GeoTIFFs\n",
    "2. **Consistent normalization** - Z-score normalization computed from training set only\n",
    "3. **Fixed train/val splits** - Reproducible splits saved to CSV files\n",
    "4. **Memory efficiency** - Load entire dataset to GPU memory for fast training\n",
    "\n",
    "The following cells run our custom preprocessing pipeline from `backend/src/data/prepare_dataset.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nj2a02wvev",
   "metadata": {},
   "source": [
    "## 6. Run the Preprocessing Pipeline\n",
    "\n",
    "This script will:\n",
    "1. Scan all tiles in `data/agrifieldnet/source/`\n",
    "2. Compute per-band normalization statistics (mean, std) from training tiles only\n",
    "3. Create 80/20 train/val split with fixed random seed (42)\n",
    "4. Generate normalized `.npy` files for fast PyTorch loading\n",
    "5. Save metadata (`class_map.json`, `normalization_stats.json`)\n",
    "\n",
    "### Output Structure:\n",
    "```\n",
    "data/\n",
    "├── processed/\n",
    "│   ├── train_images.npy          # (N_train, 12, 256, 256) float32\n",
    "│   ├── train_masks.npy           # (N_train, 256, 256) int64\n",
    "│   ├── val_images.npy            # (N_val, 12, 256, 256) float32\n",
    "│   ├── val_masks.npy             # (N_val, 256, 256) int64\n",
    "│   ├── normalization_stats.json  # Per-band mean and std\n",
    "│   └── class_map.json            # Class ID → name/color mapping\n",
    "└── splits/\n",
    "    ├── train_ids.csv             # Training tile IDs\n",
    "    └── val_ids.csv               # Validation tile IDs\n",
    "```\n",
    "\n",
    "> **Note**: This may take 10-30 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3yyqsyploy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the preprocessing pipeline\n",
    "# This imports and runs the main() function from prepare_dataset.py\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \".\")  # Add project root to path\n",
    "\n",
    "from backend.src.data.prepare_dataset import main as run_preprocessing\n",
    "\n",
    "# Run the full preprocessing pipeline\n",
    "run_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o8k8zxdts5",
   "metadata": {},
   "source": [
    "## 7. Verify the Processed Data\n",
    "\n",
    "Let's verify that the preprocessing completed successfully by:\n",
    "1. Loading the generated `.npy` files\n",
    "2. Checking array shapes and data types\n",
    "3. Verifying normalization (mean ≈ 0, std ≈ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roa97ux1c8g",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load processed data\n",
    "data_dir = Path(\"data/processed\")\n",
    "\n",
    "train_images = np.load(data_dir / \"train_images.npy\")\n",
    "train_masks = np.load(data_dir / \"train_masks.npy\")\n",
    "val_images = np.load(data_dir / \"val_images.npy\")\n",
    "val_masks = np.load(data_dir / \"val_masks.npy\")\n",
    "\n",
    "# Print shapes and dtypes\n",
    "print(\"=\" * 50)\n",
    "print(\"Dataset Shapes:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"train_images: {train_images.shape} ({train_images.dtype})\")\n",
    "print(f\"train_masks:  {train_masks.shape} ({train_masks.dtype})\")\n",
    "print(f\"val_images:   {val_images.shape} ({val_images.dtype})\")\n",
    "print(f\"val_masks:    {val_masks.shape} ({val_masks.dtype})\")\n",
    "\n",
    "# Verify normalization\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Normalization Verification (per-band):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Band':<6} {'Mean':>10} {'Std':>10}\")\n",
    "print(\"-\" * 28)\n",
    "for i in range(train_images.shape[1]):\n",
    "    mean = train_images[:, i, :, :].mean()\n",
    "    std = train_images[:, i, :, :].std()\n",
    "    print(f\"Band {i:<2} {mean:>10.4f} {std:>10.4f}\")\n",
    "\n",
    "# Load and display class map (use UTF-8 encoding for Arabic text)\n",
    "with open(data_dir / \"class_map.json\", encoding=\"utf-8\") as f:\n",
    "    class_map = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Class Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'ID':<4} {'Name':<15} {'Pixels':>12} {'Percentage':>10}\")\n",
    "print(\"-\" * 45)\n",
    "total_pixels = sum(class_map[\"class_counts\"].values())\n",
    "for cls_id, count in sorted(class_map[\"class_counts\"].items(), key=lambda x: -x[1]):\n",
    "    cls_info = class_map[\"classes\"].get(str(cls_id), {\"name\": \"Unknown\"})\n",
    "    pct = count / total_pixels * 100\n",
    "    print(f\"{cls_id:<4} {cls_info['name']:<15} {count:>12,} {pct:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rhvc4e2db3f",
   "metadata": {},
   "source": [
    "## 8. Visualize Sample Tiles\n",
    "\n",
    "Let's visualize a few sample tiles to verify the data looks correct:\n",
    "- **Left**: RGB composite (bands B04, B03, B02 = Red, Green, Blue)\n",
    "- **Right**: Crop mask with class colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gurvc42klbu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def create_rgb_composite(image, bands=(3, 2, 1)):\n",
    "    \"\"\"\n",
    "    Create RGB composite from multi-band image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: (C, H, W) normalized image array\n",
    "    - bands: tuple of band indices for (R, G, B) - default is (B04, B03, B02)\n",
    "    \n",
    "    Returns:\n",
    "    - (H, W, 3) RGB array scaled to 0-1\n",
    "    \"\"\"\n",
    "    rgb = np.stack([image[b] for b in bands], axis=-1)\n",
    "    # Rescale from normalized to 0-1 for display\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min() + 1e-6)\n",
    "    return np.clip(rgb, 0, 1)\n",
    "\n",
    "def create_mask_rgb(mask, class_colors):\n",
    "    \"\"\"\n",
    "    Create RGB visualization of class mask.\n",
    "    \n",
    "    Parameters:\n",
    "    - mask: (H, W) integer class labels\n",
    "    - class_colors: dict mapping class_id -> [R, G, B]\n",
    "    \n",
    "    Returns:\n",
    "    - (H, W, 3) RGB array scaled to 0-1\n",
    "    \"\"\"\n",
    "    h, w = mask.shape\n",
    "    rgb = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    \n",
    "    for cls_id, color in class_colors.items():\n",
    "        cls_mask = mask == int(cls_id)\n",
    "        for c in range(3):\n",
    "            rgb[:, :, c] += cls_mask * (color[c] / 255.0)\n",
    "    \n",
    "    return rgb\n",
    "\n",
    "# Get class colors from class_map\n",
    "class_colors = {int(k): v[\"color\"] for k, v in class_map[\"classes\"].items()}\n",
    "\n",
    "# Plot 4 sample tiles\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 16))\n",
    "fig.suptitle(\"Sample Training Tiles\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "sample_indices = [0, len(train_images)//4, len(train_images)//2, 3*len(train_images)//4]\n",
    "\n",
    "for row, idx in enumerate(sample_indices):\n",
    "    # RGB composite (B04=Red, B03=Green, B02=Blue -> indices 3, 2, 1)\n",
    "    rgb = create_rgb_composite(train_images[idx], bands=(3, 2, 1))\n",
    "    axes[row, 0].imshow(rgb)\n",
    "    axes[row, 0].set_title(f\"Tile {idx}: RGB Composite\")\n",
    "    axes[row, 0].axis(\"off\")\n",
    "    \n",
    "    # Mask visualization\n",
    "    mask_rgb = create_mask_rgb(train_masks[idx], class_colors)\n",
    "    axes[row, 1].imshow(mask_rgb)\n",
    "    axes[row, 1].set_title(f\"Tile {idx}: Crop Mask\")\n",
    "    axes[row, 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create legend\n",
    "print(\"\\nClass Legend:\")\n",
    "print(\"-\" * 40)\n",
    "for cls_id in sorted(class_colors.keys()):\n",
    "    cls_info = class_map[\"classes\"].get(str(cls_id), {\"name\": \"Unknown\"})\n",
    "    color = class_colors[cls_id]\n",
    "    print(f\"  {cls_id:>2}: {cls_info['name']:<15} RGB{tuple(color)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kfwyprrpy9",
   "metadata": {},
   "source": [
    "## 9. Test PyTorch Dataset Integration\n",
    "\n",
    "Finally, let's verify that our `CropDataset` class works correctly with PyTorch DataLoader.\n",
    "\n",
    "This is the interface that Student C (Training) will use to train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "siwzw4s6bx",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.src.data.dataset import CropDataset, get_dataloaders, RandomFlipRotate\n",
    "\n",
    "# Load dataset using our custom class\n",
    "train_dataset = CropDataset(\"data/processed\", split=\"train\")\n",
    "val_dataset = CropDataset(\"data/processed\", split=\"val\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CropDataset Properties:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of classes:  {train_dataset.num_classes}\")\n",
    "print(f\"Number of channels: {train_dataset.num_channels}\")\n",
    "\n",
    "# Test single sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample batch structure:\")\n",
    "print(f\"  image: {sample['image'].shape} ({sample['image'].dtype})\")\n",
    "print(f\"  mask:  {sample['mask'].shape} ({sample['mask'].dtype})\")\n",
    "\n",
    "# Test DataLoader (num_workers=0 required for Windows/Jupyter)\n",
    "train_loader, val_loader = get_dataloaders(\"data/processed\", batch_size=8, num_workers=0)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nDataLoader batch structure:\")\n",
    "print(f\"  image: {batch['image'].shape} ({batch['image'].dtype})\")\n",
    "print(f\"  mask:  {batch['mask'].shape} ({batch['mask'].dtype})\")\n",
    "\n",
    "# Test class weights (for handling class imbalance)\n",
    "weights = train_dataset.get_class_weights()\n",
    "print(f\"\\nClass weights shape: {weights.shape}\")\n",
    "print(f\"Class weights (first 5): {weights[:5].numpy()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Data Pipeline Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Student B: Implement U-Net model in backend/src/models/\")\n",
    "print(\"  2. Student C: Implement training loop in backend/src/train/\")\n",
    "print(\"  3. Student D: Implement inference API in backend/src/inference/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
