# AgroVision

**AgroVision Crop Mapper** is a user-friendly, map-based application for crop mapping using **semantic segmentation and deep learning**.
The system follows a **modern web architecture** with a **React (shadcn) frontend** and a **FastAPI backend**, enabling non-technical users to explore, analyze, and export crop maps while keeping the machine learning pipeline modular, reproducible, and scalable.

---

### ğŸ›°ï¸ What AgriFieldNet actually contains

* **256Ã—256 Sentinel-2 image chips**

* **12 spectral bands** per chip

* For each chip:

  * **Satellite imagery** (multiband TIFFs)
  * **Raster label file** (crop IDs)
  * **Field ID file** (field-level reference)

* The dataset is **already tiled**, so no manual mosaicking or tiling is required.

---

### ğŸ“‚ What this means for your project

**â†’ No fundamental change to the ML pipeline is required.**
However, the project structure is clarified to:

* separate **ML code**, **backend API**, and **frontend UI**
* support parallel work
* align with real-world deployment practices

---

## 1) Simplified file structure (clean + easy to split)

```
agrovision-crop-mapper/
â”‚
â”œâ”€ README.md
â”œâ”€ .gitignore
â”‚
â”œâ”€ config/
â”‚   â””â”€ config.yaml                 # paths, device, tile_limit, class names/colors, band list
â”‚
â”œâ”€ data/                           # local data (NOT pushed)
â”‚   â”œâ”€ raw/                        # downloaded AgriFieldNet (untouched)
â”‚   â”‚   â”œâ”€ source/
â”‚   â”‚   â”œâ”€ train_labels/
â”‚   â”‚   â””â”€ test_labels/
â”‚   â”‚
â”‚   â”œâ”€ processed/                  # PyTorch-ready (generated once)
â”‚   â”‚   â”œâ”€ train_images.npy         # (N, C, H, W)
â”‚   â”‚   â”œâ”€ train_masks.npy          # (N, H, W)
â”‚   â”‚   â”œâ”€ val_images.npy
â”‚   â”‚   â”œâ”€ val_masks.npy
â”‚   â”‚   â””â”€ class_map.json
â”‚   â”‚
â”‚   â””â”€ splits/
â”‚       â”œâ”€ train_ids.csv
â”‚       â””â”€ val_ids.csv
â”‚
â”œâ”€ backend/                        # FastAPI backend (Python)
â”‚   â”œâ”€ main.py                     # FastAPI entry point
â”‚   â”œâ”€ api/
â”‚   â”‚   â”œâ”€ routes.py               # REST endpoints (/infer, /stats, /export)
â”‚   â”‚   â””â”€ schemas.py              # Pydantic request/response models
â”‚   â”œâ”€ services/
â”‚   â”‚   â””â”€ inference_service.py    # connects API to ML inference
â”‚   â”‚
â”‚   â””â”€ src/                        # ML engine (library, NOT an app)
â”‚       â”œâ”€ data/                   # dataset + preprocessing
â”‚       â”‚   â”œâ”€ download_links.md
â”‚       â”‚   â”œâ”€ prepare_dataset.py
â”‚       â”‚   â””â”€ dataset.py
â”‚       â”‚
â”‚       â”œâ”€ models/                 # segmentation models (from scratch)
â”‚       â”‚   â”œâ”€ blocks.py            # CNN + transformer blocks
â”‚       â”‚   â”œâ”€ unet_baseline.py
â”‚       â”‚   â””â”€ unet_transformer.py
â”‚       â”‚
â”‚       â”œâ”€ train/                  # offline training + evaluation
â”‚       â”‚   â”œâ”€ train.py
â”‚       â”‚   â”œâ”€ evaluate.py
â”‚       â”‚   â””â”€ metrics.py
â”‚       â”‚
â”‚       â”œâ”€ inference/              # inference + overlay + stats
â”‚       â”‚   â”œâ”€ predictor.py
â”‚       â”‚   â”œâ”€ stitch.py
â”‚       â”‚   â””â”€ stats.py
â”‚       â”‚
â”‚       â””â”€ utils/
â”‚           â”œâ”€ io.py
â”‚           â”œâ”€ viz.py
â”‚           â””â”€ logging.py
â”‚
â”œâ”€ frontend/                       # React + shadcn UI
â”‚   â”œâ”€ package.json
â”‚   â”œâ”€ src/
â”‚   â”‚   â”œâ”€ main.tsx
â”‚   â”‚   â”œâ”€ app/                    # pages / routes
â”‚   â”‚   â”œâ”€ components/             # shadcn + custom components
â”‚   â”‚   â””â”€ lib/api.ts               # API calls to FastAPI
â”‚   â””â”€ ...
â”‚
â”œâ”€ outputs/                        # generated outputs (NOT pushed)
â”‚   â”œâ”€ runs/                       # model checkpoints, logs
â”‚   â””â”€ exports/                    # PNG / CSV generated by backend
â”‚
â””â”€ tests/
   â””â”€ test_smoke.py
```

frontend **never touches ML code directly**.


**Rule of thumb (updated):** ignore `data/raw/`, `data/processed/`, and `outputs/`. Version everything else.
(You can keep empty folders using `.gitkeep` files if you want.)

---

## 2) Divide work across 5 students (balanced, parallel, minimal dependency) â€” **UPDATED (FastAPI + React)**

### Student A â€” Data pipeline (prep + dataset loader)

**Owns:**
`backend/src/data/*`, `config/config.yaml`, `data/splits/*`, `data/processed/*` (generated locally)

* Download instructions + folder organization (`data/raw/...`)
* `prepare_dataset.py` to generate:

  * `data/processed/*.npy`
  * `data/processed/class_map.json`
  * optional `data/splits/*.csv`
* `dataset.py` that loads from `data/processed/` (fast, PyTorch-ready)
* Defines/maintains **band list + normalization** in `config.yaml`

**Delivers:**
â€œRun one script â†’ processed data exists â†’ training loads batches correctly.â€


### Student B â€” Model implementation (baseline + custom transformer blocks)

**Owns:**
`backend/src/models/*`

* Build baseline **U-Net from scratch** (`unet_baseline.py`)
* Implement reusable components (`blocks.py`) including:

  * CNN blocks (Convâ€“BNâ€“ReLU, down/up blocks)
  * **Transformer blocks** (as required by the course)
* Build custom architecture using transformer blocks (`unet_transformer.py`)
* Ensure configurable:

  * `in_channels = C`
  * `num_classes = K`
* Dummy test with shapes:

  * input: `(B, C, 256, 256)`
  * output: `(B, K, 256, 256)`

**Delivers:**
â€œExplains architectural choices + passes dummy tensor test + ready to plug into training.â€

### Student C â€” Training + evaluation

**Owns:**
`backend/src/train/*`

* Training loop + checkpointing to `outputs/runs/`
* Losses: Cross-Entropy + optional Dice
* Metrics: mIoU, per-class IoU, macro F1
* Evaluation script + metric reports

**Delivers:**
â€œTrain baseline/custom model and produce reproducible metrics + best checkpoint.â€

### Student D â€” Backend inference + API (FastAPI)

**Owns:**
`backend/main.py`, `backend/api/*`, `backend/services/*`,
and `backend/src/inference/*` (plus `backend/src/utils/*` if needed)

* Build **FastAPI backend**
* API endpoints (example):

  * `POST /api/infer` â†’ returns overlay + stats
  * `GET /api/legend` â†’ class names + colors
  * `POST /api/export` (optional)
* `inference_service.py` bridges API â†’ ML inference
* Load trained model once (cached in backend)
* Call `backend/src/inference/*` for:

  * prediction
  * stitching (if needed)
  * stats computation

**Delivers:**
â€œFrontend can call backend API and receive overlay + stats reliably.â€

### Student E â€” Frontend (React + shadcn)

**Owns:**
`frontend/src/*`

* React UI using **shadcn components**
* Map-like AOI selection concept
* â€œRun Analysisâ€ button â†’ calls `POST /api/infer`
* Opacity slider, legend, stats table
* Export buttons (PNG + CSV via backend)
* UX states: loading, disabled (tile limit), errors

**Delivers:**
â€œWorking frontend that consumes backend API and displays/exports results.â€

## 3) Maximize parallel work (dependency map + how to avoid blocking)

### Minimal dependency chain

* **A (Data)** and **B (Model)** start immediately in parallel.
* **C (Training)** can start with **synthetic/dummy data** before A finishes.
* **D (Backend)** can start with **mock inference outputs** before training finishes.
* **E (Frontend)** can start with **mock API responses** before backend is complete.

â¡ï¸ No student waits for another.

### Practical â€œmock-firstâ€ approach (so nobody blocks)

* Define a **shared API contract early**:

  ```text
  POST /api/infer
  â†’ overlay_image (base64 or URL)
  â†’ stats_table (JSON)
  â†’ raw_mask (optional)
  ```
* Student D implements mock API responses first
* Student E builds UI against mock API
* When Student C produces `best_model.pth`, Student D switches mock â†’ real inference

### Weekly-style parallel milestones

* **Milestone 1:**
  Dataset prep works + model runs on dummy input + frontend UI renders mock results
* **Milestone 2:**
  Training produces checkpoint + backend uses real model + frontend fully wired
* **Milestone 3:**
  Evaluation report + exports + README + demo polish

### One-line team rule (recommended to include)

> **Backend owns ML. Frontend owns UI.
> Communication happens only through API calls.
> Training is offline and never triggered from the frontend.**
Below are the **updated versions of ONLY the parts you pasted**, rewritten to match
ğŸ‘‰ **React + shadcn (frontend)**
ğŸ‘‰ **FastAPI (backend)**
ğŸ‘‰ **NO Streamlit, NO `src/app/`**

Each section is **copy-paste ready**.

---

## 1) Where is the access for the app? (Entry points)

### âœ… **Two entry points (Frontend + Backend)**

### Backend (FastAPI)

```
backend/main.py
```

This is the **only Python file that is run**.

**Example:**

```bash
uvicorn backend.main:app --reload
```

The backend:

* loads the trained model
* runs inference
* exposes REST APIs for the frontend

---

### Frontend (React + shadcn)

```
frontend/src/main.tsx
```

**Example:**

```bash
cd frontend
npm install
npm run dev
```

â¡ï¸ The frontend **never runs ML code**.
â¡ï¸ It only communicates with the backend via HTTP APIs.


## 2) How the project is split across students (very important)

> **Key rule:**
> Each student owns **one folder** (backend ML, backend API, or frontend).
> ML code lives inside the backend and is **imported**, not executed directly.

### Folder ownership

```
backend/src/
â”œâ”€ data/        â†’ Student A (data pipeline)
â”œâ”€ models/      â†’ Student B (model architecture)
â”œâ”€ train/       â†’ Student C (training & evaluation)
â”œâ”€ inference/   â†’ Student D (prediction + stats)
â””â”€ utils/       â†’ shared helpers

backend/
â”œâ”€ api/         â†’ Student D (FastAPI routes)
â”œâ”€ services/    â†’ Student D (connect API to ML)
â””â”€ main.py      â†’ backend entry point

frontend/
â””â”€ src/         â†’ Student E (React + shadcn UI)
```

### What this means in practice

* Students **do not edit each otherâ€™s folders**
* ML code is reused by importing it into FastAPI
* Frontend and backend work **independently**
* Integration happens via **API contracts**, not shared files


## 3) High-level connection (one-page mental model)

```
User
 â†“
Frontend (React + shadcn)
 â†“   HTTP (JSON)
Backend (FastAPI)
 â†“
Inference Engine
 â†“
Model
 â†“
Stats + Overlay
 â†“
Backend returns JSON + image
 â†“
Frontend displays + exports
```

Or in folders:

```
frontend/
   â†“ (API calls)
backend/
   â”œâ”€ api/
   â”œâ”€ services/
   â””â”€ src/
       â”œâ”€ inference/
       â”œâ”€ models/
       â””â”€ utils/
```

Training is **separate and offline**:

```
backend/src/train/ â†’ backend/src/models/
backend/src/train/ â†’ backend/src/data/
```


## 4) Detailed connection: who calls whom (important)

## A) Frontend (React) â€” Student E

**Role:**

* UI only (map selection, buttons, sliders, tables)
* No ML logic
* No direct access to model or data

**What it does:**

1. User selects region (AOI)
2. User clicks **Run Analysis**
3. Sends request to backend API (`POST /api/infer`)
4. Receives:

   * overlay image (URL or base64)
   * stats table (JSON)
5. Displays results + export options


## B) Backend (FastAPI) â€” Student D

### `backend/api/routes.py`

**Role:** define REST endpoints

Examples:

* `POST /api/infer`
* `GET /api/legend`
* `POST /api/export`


### `backend/services/inference_service.py`

**Role:** bridge between API and ML code

```python
from src.inference.predictor import run_inference
```

* Loads trained model once (cached)
* Calls ML inference functions
* Formats results for API response


### `backend/src/inference/`

**Role:** pure inference logic (NO FastAPI here)

* `predictor.py` â†’ run model inference
* `stitch.py` â†’ combine tiles
* `stats.py` â†’ compute pixel counts, percentages, confidence

```python
def run_inference(viewport_tiles):
    mask = predict_tiles(viewport_tiles)
    overlay = make_overlay(mask)
    stats = compute_stats(mask)
    return overlay, stats
```

â¡ï¸ **This function is the API between ML and backend.**


## C) `models/` (pure deep learning) â€” Student B

### `unet_baseline.py / unet_transformer.py`

* ONLY PyTorch code
* No file I/O
* No API or UI logic
* Built from scratch (baseline + transformer blocks)

Used by:

* `backend/src/train/train.py`
* `backend/src/inference/predictor.py`


## D) `train/` (offline, not part of runtime) â€” Student C

### `train.py`

**Uses:**

```python
from src.data.dataset import CropDataset
from src.models.unet_baseline import UNet
```

**Produces:**

```
outputs/runs/best_model.pth
```

â¡ï¸ The backend **loads this file**.
â¡ï¸ Training is never triggered from the frontend.


## E) `data/` (used only by training) â€” Student A

### `prepare_dataset.py`

* Converts raw TIFFs â†’ `.npy`
* Creates `data/processed/`

### `dataset.py`

* PyTorch `Dataset`
* Loads from `data/processed/`

â¡ï¸ Neither backend APIs nor frontend touch raw data.

## F) `config/config.yaml` (glue)

Used by:

* training
* backend inference

Contains:

```yaml
model_path: outputs/runs/best_model.pth
tile_limit: 9
class_names: [...]
class_colors: [...]
device: cuda
```

â¡ï¸ Change behavior without touching frontend or backend logic.


## 5) One full execution trace (step-by-step)

### User runs (two terminals)

**Backend**

```bash
uvicorn backend.main:app --reload
```

**Frontend**

```bash
cd frontend
npm install
npm run dev
```

### Runtime flow

1. Frontend loads UI
2. User selects AOI
3. User clicks **Run Analysis**
4. Frontend sends request to backend (`POST /api/infer`)
5. Backend loads model once (cached)
6. Backend runs inference + stats
7. Backend returns JSON + overlay image
8. Frontend renders overlay, stats, and export buttons


## 6) Why this structure is correct (and safe for grading)

* **Clear frontend/backend separation**
* **Industry-standard architecture**
* **No ML code in the frontend**
* **No UI code in the ML layer**
* **Clear ownership per student**
* **Easy to mock APIs**
* **Scales to real deployment**