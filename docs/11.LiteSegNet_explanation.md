## Architecture: **LiteFieldSeg**

**Goal:** keep the same pixel-wise output interface as U-Net (`[B, C, H, W]`) but be **faster + lower RAM** while still having **attention**.

### Key ideas (why it’s lighter than U-Net)

1. **Depthwise-separable convs** (MobileNet-style) replace heavy full convolutions → big FLOPs/RAM reduction.
2. **Inverted residual blocks (MobileNetV2 / EfficientNet-inspired)** → efficient feature extraction.
3. **Skip fusion uses addition (FPN-style)** instead of U-Net concatenation → avoids doubling channels (major RAM win).
4. **Attention included** via:

    - **SE (Squeeze-and-Excitation)** inside each encoder block (channel attention)
    - **Lite CBAM** on the high-res fused feature (channel + spatial attention) for boundary refinement

---

# Minimal PyTorch Code (single file)

```python
"""
LiteFieldSeg: a lightweight segmentation network for sparse-labeled AgriFieldNet.

Design goals:
- Faster + lower memory than U-Net (depthwise convs, skip-add not concat)
- Attention modules included (SE in encoder, CBAM-lite in decoder)
- Output is pixel-wise logits [B, num_classes, H, W] for overlay + field-aware training
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Tuple

import torch
from torch import nn
from torch.nn import functional as F


# -------------------------
# Small reusable blocks
# -------------------------

class ConvBNAct(nn.Module):
    """Conv -> BN -> activation. (Used for 1x1 and regular convs.)"""

    def __init__(
        self,
        in_ch: int,
        out_ch: int,
        k: int = 3,
        s: int = 1,
        p: int | None = None,
        groups: int = 1,
        act: str = "silu",
    ) -> None:
        super().__init__()
        if p is None:
            p = k // 2
        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, groups=groups, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)
        if act == "relu":
            self.act = nn.ReLU(inplace=True)
        elif act == "silu":
            self.act = nn.SiLU(inplace=True)
        else:
            raise ValueError(f"Unknown activation: {act}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.act(self.bn(self.conv(x)))


class DepthwiseSeparableConv(nn.Module):
    """
    Depthwise 3x3 + Pointwise 1x1.
    Much cheaper than a full 3x3 conv.
    """

    def __init__(self, in_ch: int, out_ch: int, stride: int = 1, act: str = "silu") -> None:
        super().__init__()
        self.dw = ConvBNAct(in_ch, in_ch, k=3, s=stride, groups=in_ch, act=act)
        self.pw = ConvBNAct(in_ch, out_ch, k=1, s=1, p=0, act=act)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.pw(self.dw(x))


class SEBlock(nn.Module):
    """
    Squeeze-and-Excitation (channel attention).
    Efficient attention: global pooling + tiny MLP.
    """

    def __init__(self, channels: int, reduction: int = 4) -> None:
        super().__init__()
        hidden = max(8, channels // reduction)
        self.fc1 = nn.Conv2d(channels, hidden, kernel_size=1)
        self.fc2 = nn.Conv2d(hidden, channels, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Squeeze: global average pooling
        s = F.adaptive_avg_pool2d(x, 1)
        # Excite: channel gates
        s = F.silu(self.fc1(s))
        s = torch.sigmoid(self.fc2(s))
        return x * s


class InvertedResidualSE(nn.Module):
    """
    MobileNetV2/EfficientNet-inspired inverted residual block with SE attention.
    - expand (1x1) -> depthwise (3x3) -> SE -> project (1x1)
    """

    def __init__(self, in_ch: int, out_ch: int, stride: int, expand: int = 2) -> None:
        super().__init__()
        if stride not in (1, 2):
            raise ValueError("stride must be 1 or 2")

        mid = in_ch * expand
        self.use_res = (stride == 1 and in_ch == out_ch)

        layers: List[nn.Module] = []
        if expand != 1:
            layers.append(ConvBNAct(in_ch, mid, k=1, s=1, p=0))

        # depthwise
        layers.append(ConvBNAct(mid, mid, k=3, s=stride, groups=mid))

        # attention (channel)
        layers.append(SEBlock(mid))

        # project
        layers.append(nn.Conv2d(mid, out_ch, kernel_size=1, bias=False))
        layers.append(nn.BatchNorm2d(out_ch))

        self.block = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        y = self.block(x)
        return x + y if self.use_res else y


class CBAMLite(nn.Module):
    """
    Lightweight CBAM-style attention for refinement:
    - Channel attention (SE-like)
    - Spatial attention (avg/max over channels -> small conv -> sigmoid)
    """

    def __init__(self, channels: int) -> None:
        super().__init__()
        self.channel = SEBlock(channels, reduction=4)
        self.spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.channel(x)

        # Spatial gate: pool along channel dimension
        avg = torch.mean(x, dim=1, keepdim=True)
        mx, _ = torch.max(x, dim=1, keepdim=True)
        a = torch.sigmoid(self.spatial(torch.cat([avg, mx], dim=1)))
        return x * a


# -------------------------
# Model
# -------------------------

@dataclass
class EncoderStageCfg:
    out_ch: int
    num_blocks: int
    stride: int
    expand: int


class LiteFieldSeg(nn.Module):
    """
    LiteFieldSeg
    - Encoder: inverted residual + SE (MobileNetV2/EfficientNet inspired)
    - Decoder: FPN-like top-down fusion with skip-add (not concat) + CBAM-lite refine
    - Head: produces pixel logits [B, num_classes, H, W]
    """

    def __init__(
        self,
        in_channels: int,
        num_classes: int,
        stem_ch: int = 16,
        dec_ch: int = 64,
        dropout: float = 0.0,
    ) -> None:
        super().__init__()
        self.in_channels = in_channels
        self.num_classes = num_classes

        # Early downsample reduces compute quickly
        self.stem = ConvBNAct(in_channels, stem_ch, k=3, s=2)  # /2

        # Encoder configuration (keep it modest for speed/RAM)
        cfgs = [
            EncoderStageCfg(out_ch=24, num_blocks=2, stride=2, expand=2),  # /4
            EncoderStageCfg(out_ch=32, num_blocks=3, stride=2, expand=2),  # /8
            EncoderStageCfg(out_ch=64, num_blocks=3, stride=2, expand=2),  # /16
            EncoderStageCfg(out_ch=96, num_blocks=2, stride=1, expand=2),  # /16 refine
        ]

        self.stages = nn.ModuleList()
        in_ch = stem_ch
        for c in cfgs:
            blocks: List[nn.Module] = []
            # first block may downsample
            blocks.append(InvertedResidualSE(in_ch, c.out_ch, stride=c.stride, expand=c.expand))
            in_ch = c.out_ch
            # remaining blocks keep resolution
            for _ in range(c.num_blocks - 1):
                blocks.append(InvertedResidualSE(in_ch, c.out_ch, stride=1, expand=c.expand))
            self.stages.append(nn.Sequential(*blocks))

        # Decoder (FPN-style): unify channels then fuse by addition (cheaper than concat)
        self.lat4 = ConvBNAct(24, dec_ch, k=1, s=1, p=0)
        self.lat8 = ConvBNAct(32, dec_ch, k=1, s=1, p=0)
        self.lat16 = ConvBNAct(96, dec_ch, k=1, s=1, p=0)

        self.fuse8 = DepthwiseSeparableConv(dec_ch, dec_ch)
        self.fuse4 = DepthwiseSeparableConv(dec_ch, dec_ch)

        # Attention refine near high-res output
        self.refine = CBAMLite(dec_ch)

        self.drop = nn.Dropout2d(p=dropout) if dropout > 0 else nn.Identity()
        self.pred = nn.Conv2d(dec_ch, num_classes, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Returns:
            logits: [B, num_classes, H, W]
        """
        b, _, h, w = x.shape

        x = self.stem(x)          # /2
        x4 = self.stages[0](x)    # /4  (24ch)
        x8 = self.stages[1](x4)   # /8  (32ch)
        x16 = self.stages[2](x8)  # /16 (64ch)
        x16 = self.stages[3](x16) # /16 (96ch)

        # Top-down fusion (FPN-like)
        p16 = self.lat16(x16)  # [B, dec_ch, H/16, W/16]

        p8 = self.lat8(x8) + F.interpolate(p16, size=x8.shape[-2:], mode="bilinear", align_corners=False)
        p8 = self.fuse8(p8)

        p4 = self.lat4(x4) + F.interpolate(p8, size=x4.shape[-2:], mode="bilinear", align_corners=False)
        p4 = self.fuse4(p4)

        # Attention refinement at higher resolution
        p4 = self.refine(p4)
        p4 = self.drop(p4)

        # Upsample back to input size
        p = F.interpolate(p4, size=(h, w), mode="bilinear", align_corners=False)
        logits = self.pred(p)
        return logits


# -------------------------
# Optional helper: field aggregation (useful for eval/debug)
# -------------------------

@torch.no_grad()
def aggregate_field_logits(
    logits: torch.Tensor, reveals: torch.Tensor, field_ids: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Aggregate pixel logits to per-field logits by mean over pixels.

    Args:
        logits: [B, C, H, W]
        field_ids: [B, H, W] int, 0 = no-field

    Returns:
        field_logits: [N_fields, C]
        field_batch_ids: [N_fields] which batch index each field came from
    """
    b, c, h, w = logits.shape
    flat_logits = logits.permute(0, 2, 3, 1).reshape(b, h * w, c)  # [B, HW, C]
    flat_ids = field_ids.reshape(b, h * w)                        # [B, HW]

    all_field_logits = []
    all_batch_ids = []

    for bi in range(b):
        ids = flat_ids[bi]
        lg = flat_logits[bi]

        uniq = torch.unique(ids)
        uniq = uniq[uniq > 0]  # ignore background/no-field

        for fid in uniq:
            mask = (ids == fid)
            if mask.any():
                all_field_logits.append(lg[mask].mean(dim=0))
                all_batch_ids.append(torch.tensor(bi, device=logits.device))

    if len(all_field_logits) == 0:
        return torch.empty(0, c, device=logits.device), torch.empty(0, device=logits.device, dtype=torch.long)

    return torch.stack(all_field_logits, dim=0), torch.stack(all_batch_ids, dim=0)


def _smoke_test() -> None:
    model = LiteFieldSeg(in_channels=12, num_classes=14, dropout=0.1)
    x = torch.randn(2, 12, 256, 256)
    y = model(x)
    print("input:", x.shape, "output:", y.shape)


if __name__ == "__main__":
    _smoke_test()
```

---

# Documentation (what/why/how)

## 1) Why this instead of U-Net (speed + RAM)

U-Net is expensive mainly because:

-   It uses **many full 3×3 convolutions** at multiple scales.
-   Decoder uses **skip concatenation**, which **doubles channels** at each upsample step → high activation memory.

**LiteFieldSeg reduces cost by design:**

-   Uses **depthwise-separable convs** (MobileNet-style) → far fewer multiply-adds and fewer parameters.
-   Uses **inverted residual blocks** (MobileNetV2 / EfficientNet idea) → strong accuracy/efficiency tradeoff.
-   Uses **FPN-style skip-add fusion** instead of concat → avoids big channel growth and reduces RAM.

This is an architectural optimization (not just training tricks).

---

## 2) Attention modules (required)

We add attention in two places:

### A) SE inside every encoder block (Channel Attention)

**Inspired by:** Squeeze-and-Excitation Networks / EfficientNet MBConv.
**Why:** SE is very cheap and improves class separation by reweighting channels based on global context—useful when labels are sparse and fields are small.

### B) CBAM-lite in the decoder (Channel + Spatial Attention)

**Inspired by:** CBAM, but simplified.
**Why:** The decoder feature (`/4`) is where boundary details live. A light spatial gate helps suppress noise and sharpen edges, improving overlay quality without a big compute hit.

---

## 3) Block-by-block explanation

### Stem (`ConvBNAct`, stride=2)

Downsamples early to reduce compute fast. Sentinel-2 tiles are big; this immediately cuts FLOPs ~4×.

### Encoder (`InvertedResidualSE`)

Each block:

1. **Expand 1×1** (optional): increases channels cheaply
2. **Depthwise 3×3**: captures spatial patterns at low cost
3. **SE attention**: highlights useful channels for crop types
4. **Project 1×1**: returns to compact channel width
5. Residual connection when shape matches

This is the “MobileNetV2/EfficientNet core idea” adapted to segmentation.

### Decoder (FPN-style skip-add)

We keep three scales:

-   `/4` (fine detail)
-   `/8` (mid)
-   `/16` (semantic/context)

We do:

-   1×1 lateral conv to unify channels → `dec_ch`
-   top-down upsample and **add** features (no concat)
-   small depthwise-separable conv to clean fusion artifacts

This is “FPN-inspired” but minimal.

### Head

Upsample fused `/4` feature to full resolution and apply a `1×1` conv to get final class logits.

---

## 4) How this fits AgriFieldNet + field-aware training

This model **does not change** your training interface:

-   Output logits: `[B, num_classes, H, W]` ✅
-   Your pixel overlay stays: `argmax(softmax(logits))` ✅
-   Your **CombinedLoss** stays unchanged:

    -   Pixel loss (focal CE, `ignore_index=0`)
    -   Field loss (aggregate per `field_id` > 0)

Why it helps your failure mode (“predict all background”):

-   The architecture itself is not the full fix (your dual loss is), BUT:

    -   lighter model + skip-add reduces tendency to “memorize background texture”
    -   attention helps emphasize faint crop signals within sparse labeled regions
    -   FPN fusion keeps semantics without heavy decoder capacity

---

## 5) Inspirations used (and why)

-   **MobileNetV2 / EfficientNet (MBConv + SE)**: best-known lightweight conv backbone pattern.
-   **SE Networks**: cheap attention that usually improves class discrimination.
-   **FPN**: skip **addition** is more memory-friendly than U-Net skip concatenation.
-   **CBAM**: spatial attention can improve boundaries; we use a lite version.

---

## 6) Integration checklist (drop-in replacement)

-   Replace `UNet(...)` with `LiteFieldSeg(in_channels=12, num_classes=14)`
-   Keep the same training loop and losses
-   Keep field-aware validation metric selection (`val_field_ce`)
Below is a **study-style, detailed walkthrough** of the code you got (blocks first, then the whole model forward path). I’ll explain what each line/block is doing, what shapes look like, and why it exists.

---

# Detailed Code Explanation: LiteFieldSeg

## Big picture

We want a segmentation model that outputs:

* **Input:** `x` → `[B, Cin, H, W]` (e.g., `[2, 12, 256, 256]`)
* **Output:** `logits` → `[B, Cclasses, H, W]` (e.g., `[2, 14, 256, 256]`)

But we want it to be **lighter than U-Net**, so:

* Use **depthwise / separable convs** (cheap)
* Use **skip-add** instead of **skip-concat** (saves memory)
* Add **attention** (SE + CBAM-lite)

---

# 1) Building Blocks

## 1.1 `ConvBNAct`

```python
class ConvBNAct(nn.Module):
    """Conv -> BN -> activation."""
```

### What it represents

This is the classic “convolution block” used everywhere:

1. `Conv2d`
2. `BatchNorm2d`
3. Activation (SiLU or ReLU)

### Why it exists

You’ll use this pattern many times. Instead of rewriting 3 layers repeatedly, wrap it once.

### Key parameters

```python
def __init__(self, in_ch, out_ch, k=3, s=1, p=None, groups=1, act="silu"):
```

* `k`: kernel size (3 or 1 are most common)
* `s`: stride (2 means downsampling)
* `p`: padding (defaults to `k//2` so output keeps size if `s=1`)
* `groups`: controls grouped convolution

  * `groups=1` → normal conv
  * `groups=in_ch` → **depthwise conv**
* activation: SiLU is a good default (used in EfficientNet-like nets)

### Forward

```python
return self.act(self.bn(self.conv(x)))
```

So shape changes depend on `stride` and kernel:

* If `s=1`: spatial stays `(H,W)`
* If `s=2`: spatial becomes `(H/2, W/2)`

---

## 1.2 `DepthwiseSeparableConv`

```python
class DepthwiseSeparableConv(nn.Module):
    """
    Depthwise 3x3 + Pointwise 1x1.
    Much cheaper than a full 3x3 conv.
    """
```

### The concept

A standard conv mixes:

* spatial info (3×3)
* channel mixing (Cin→Cout)
  in **one expensive operation**.

Depthwise separable splits this into:

1. **Depthwise conv:** one 3×3 filter per input channel
2. **Pointwise conv:** a 1×1 conv that mixes channels

### In code

```python
self.dw = ConvBNAct(in_ch, in_ch, k=3, s=stride, groups=in_ch)
self.pw = ConvBNAct(in_ch, out_ch, k=1, s=1, p=0)
```

* `groups=in_ch` makes it depthwise.
* The pointwise conv is where channels become `out_ch`.

### Why it matters

This is one of the main reasons LiteFieldSeg is faster than U-Net.

---

## 1.3 `SEBlock` (Squeeze-and-Excitation)

```python
class SEBlock(nn.Module):
    """
    Squeeze-and-Excitation (channel attention).
    Efficient attention: global pooling + tiny MLP.
    """
```

### What SE does

SE learns a per-channel importance mask.

Steps:

1. **Squeeze:** global average pool → `[B, C, 1, 1]`
2. **Excite:** small MLP (implemented by 1×1 convs) → `[B, C, 1, 1]`
3. Multiply it back into the feature map.

### Why it helps

Channels often correspond to certain patterns (edges, textures, spectral patterns).
SE can amplify channels useful for crops and suppress noisy ones.

### In code

```python
s = F.adaptive_avg_pool2d(x, 1)  # squeeze
s = F.silu(self.fc1(s))
s = torch.sigmoid(self.fc2(s))   # gates in [0,1]
return x * s
```

Shapes:

* Input `x`: `[B, C, H, W]`
* After pooling `s`: `[B, C, 1, 1]`
* After gates: `[B, C, 1, 1]`
* Multiply broadcasts over H,W

---

## 1.4 `InvertedResidualSE`

```python
class InvertedResidualSE(nn.Module):
    """
    MobileNetV2/EfficientNet-inspired inverted residual block with SE attention.
    - expand (1x1) -> depthwise (3x3) -> SE -> project (1x1)
    """
```

### The core efficiency trick

Instead of doing expensive convs at a big output channel count all the time:

* expand temporarily (cheap 1×1)
* do cheap depthwise spatial conv
* project back down

### Residual connection rule

```python
self.use_res = (stride == 1 and in_ch == out_ch)
```

Residual only works if:

* no downsampling (stride=1)
* same channels

### Layers inside

```python
if expand != 1:
    layers.append(ConvBNAct(in_ch, mid, k=1))

layers.append(ConvBNAct(mid, mid, k=3, s=stride, groups=mid))  # depthwise
layers.append(SEBlock(mid))                                    # attention

layers.append(nn.Conv2d(mid, out_ch, kernel_size=1, bias=False)) # project
layers.append(nn.BatchNorm2d(out_ch))
```

Note: final projection has **no activation** (MobileNetV2 style).
This makes the residual behavior more stable.

### Forward

```python
y = self.block(x)
return x + y if self.use_res else y
```

---

## 1.5 `CBAMLite`

```python
class CBAMLite(nn.Module):
    """
    Lightweight CBAM-style attention for refinement:
    - Channel attention (SE-like)
    - Spatial attention (avg/max over channels -> small conv -> sigmoid)
    """
```

### What it does

CBAM = Channel attention + Spatial attention.

We already have channel attention via `SEBlock`, so we reuse it:

```python
x = self.channel(x)
```

Then spatial attention:

1. average across channels → `[B, 1, H, W]`
2. max across channels → `[B, 1, H, W]`
3. concat → `[B, 2, H, W]`
4. 7×7 conv → `[B, 1, H, W]`
5. sigmoid → spatial mask
6. multiply back

```python
avg = torch.mean(x, dim=1, keepdim=True)
mx, _ = torch.max(x, dim=1, keepdim=True)
a = torch.sigmoid(self.spatial(torch.cat([avg, mx], dim=1)))
return x * a
```

### Why we apply it in decoder

Decoder features at `/4` resolution contain boundaries and fine structures.
Spatial attention helps emphasize crop regions and suppress clutter.

---

# 2) The Model: LiteFieldSeg

## 2.1 Stem

```python
self.stem = ConvBNAct(in_channels, stem_ch, k=3, s=2)  # /2
```

If input is `[B, 12, 256, 256]` and `stem_ch=16`:

* Output becomes `[B, 16, 128, 128]`

This early downsample is a major compute saver.

---

## 2.2 Encoder stages

Config:

```python
cfgs = [
    out=24, blocks=2, stride=2  # /4
    out=32, blocks=3, stride=2  # /8
    out=64, blocks=3, stride=2  # /16
    out=96, blocks=2, stride=1  # /16 refine
]
```

### What this means

We progressively shrink spatial resolution:

* `/2` after stem
* `/4` after stage0
* `/8` after stage1
* `/16` after stage2
* stage3 keeps `/16` but increases representation richness

Why: deeper layers need bigger receptive field and more context.

---

## 2.3 Decoder (FPN-style skip-add)

U-Net decoder does:

* upsample + concat skip + heavy convs

We do a cheaper version:

* 1×1 lateral conv to unify channels to `dec_ch`
* upsample + **add** skip
* small depthwise-separable conv to clean

### Lateral projections

```python
self.lat4 = ConvBNAct(24, dec_ch, k=1)
self.lat8 = ConvBNAct(32, dec_ch, k=1)
self.lat16 = ConvBNAct(96, dec_ch, k=1)
```

This makes all scales have the same channel count, so add works.

---

## 2.4 Forward pass with shapes

Assume:

* `x`: `[B, 12, 256, 256]`
* `stem_ch=16`, `dec_ch=64`

### Step 1: stem

```python
x = self.stem(x)  # [B, 16, 128, 128]
```

### Step 2: encoder

```python
x4  = stage0(x)   # [B, 24,  64,  64]
x8  = stage1(x4)  # [B, 32,  32,  32]
x16 = stage2(x8)  # [B, 64,  16,  16]
x16 = stage3(x16) # [B, 96,  16,  16]
```

### Step 3: top-down fusion

Start from lowest res:

```python
p16 = lat16(x16)  # [B, 64, 16, 16]
```

Fuse into `/8`:

```python
p8 = lat8(x8) + up(p16 to 32x32)  # [B, 64, 32, 32]
p8 = fuse8(p8)                    # still [B, 64, 32, 32]
```

Fuse into `/4`:

```python
p4 = lat4(x4) + up(p8 to 64x64)   # [B, 64, 64, 64]
p4 = fuse4(p4)                    # [B, 64, 64, 64]
```

### Step 4: refine attention + dropout

```python
p4 = refine(p4)  # CBAM-lite attention, same shape
p4 = drop(p4)
```

### Step 5: upsample to full resolution and predict

```python
p = up(p4 to 256x256)        # [B, 64, 256, 256]
logits = pred(p)             # [B, 14, 256, 256]
```

This matches your pipeline exactly.

---

# 3) Optional helper: field aggregation

## `aggregate_field_logits`

This is a debugging/evaluation helper, not required for training.

It converts pixel logits into per-field logits:

* Flatten pixel grid (`H*W`)
* For each `field_id > 0`:

  * select pixels belonging to it
  * take mean of logits over those pixels

So each field counts **once**, matching challenge evaluation logic.

---

# 4) Why these design choices match your AgriFieldNet problem

### Sparse labels + field loss

Your training already solves the main issue (wrong objective) using field-aware loss.
This architecture helps because:

* It is **efficient**, so you can train longer / bigger batches / more augmentation.
* It has **attention**, so it can better focus on weak crop signals within sparse labeled areas.
* FPN-style skip-add keeps fine details without U-Net’s heavy memory cost.

---

# 5) How to study it effectively (suggested approach)

1. Run `_smoke_test()` and print shapes at each step (add temporary prints).
2. Comment out attention blocks (SE, CBAMLite) and compare:

   * speed
   * stability
   * validation field CE
3. Compare skip-add vs skip-concat by making a mini decoder variant.

Here’s a **diagram-style explanation** of **LiteFieldSeg** (with shapes) in the same “box + arrows” style you used in your training doc.

---

# LiteFieldSeg Diagram (Encoder–Decoder + Attention)

Assume example input:

* `Cin = 12` (Sentinel-2 bands)
* `H=W=256`
* `num_classes = 14`
* `stem_ch=16`, `dec_ch=64`

---

## 1) High-level pipeline

```
Input image tile
x: [B, 12, 256, 256]
        |
        v
+-----------------------+
| Stem: ConvBNAct s=2   |   (early downsample for speed)
| 3x3 Conv -> BN -> SiLU|
+-----------------------+
        |
        v
x2: [B, 16, 128, 128]
        |
        v
+-----------------------+     +-----------------------+     +-----------------------+
| Encoder Stage /4      | --> | Encoder Stage /8      | --> | Encoder Stage /16     |
| InvRes + SE (stride2) |     | InvRes + SE (stride2) |     | InvRes + SE (stride2) |
| out=24                |     | out=32                |     | out=64 -> refine->96  |
+-----------------------+     +-----------------------+     +-----------------------+
        |                         |                         |
        |                         |                         |
        v                         v                         v
x4: [B,24,64,64]           x8: [B,32,32,32]           x16: [B,96,16,16]
        |                         |                         |
        +-----------+-------------+-------------+-----------+
                    |                           |
                    v                           v
               (Decoder / FPN-style top-down fusion with skip-add)
```

---

## 2) Encoder detail (what “InvRes + SE” means)

Each encoder block is:

```
InvertedResidualSE Block

x: [B, Cin, H, W]
   |
   |  (optional) 1x1 expand (cheap)
   v
[B, Cin*expand, H, W]
   |
   |  3x3 depthwise conv (groups=channels)  <-- very cheap spatial conv
   v
[B, Cin*expand, H/stride, W/stride]
   |
   |  SE Attention (channel reweight)
   v
[B, Cin*expand, H/stride, W/stride]
   |
   |  1x1 project back to out_ch
   v
y: [B, Cout, H/stride, W/stride]

Residual add only if (stride=1 and Cin=Cout):
output = x + y
```

**Why this helps:** depthwise conv saves compute; SE helps crop class discrimination.

---

## 3) Decoder diagram (FPN-style skip-add, not U-Net concat)

### Step A — “Lateral” 1×1 projections (unify channels to `dec_ch=64`)

```
x16: [B, 96, 16, 16] --(1x1)-> p16: [B, 64, 16, 16]
x8 : [B, 32, 32, 32] --(1x1)-> l8 : [B, 64, 32, 32]
x4 : [B, 24, 64, 64] --(1x1)-> l4 : [B, 64, 64, 64]
```

### Step B — Top-down fusion with **upsample + add**

```
p16: [B,64,16,16]
   |
   | upsample to 32x32
   v
up(p16): [B,64,32,32]

p8 = l8 + up(p16)
p8: [B,64,32,32]
   |
   | DepthwiseSeparableConv (cleanup)
   v
p8': [B,64,32,32]
```

Then fuse into `/4`:

```
p8': [B,64,32,32]
   |
   | upsample to 64x64
   v
up(p8'): [B,64,64,64]

p4 = l4 + up(p8')
p4: [B,64,64,64]
   |
   | DepthwiseSeparableConv (cleanup)
   v
p4': [B,64,64,64]
```

### Step C — Attention refinement near high resolution

```
p4': [B,64,64,64]
   |
   | CBAM-Lite attention:
   |  - Channel gate (SE)
   |  - Spatial gate (avg/max -> 7x7 conv -> sigmoid)
   v
p4_refined: [B,64,64,64]
```

### Step D — Upsample to full resolution + predict classes

```
p4_refined: [B,64,64,64]
   |
   | upsample to 256x256
   v
p: [B,64,256,256]
   |
   | 1x1 conv head
   v
logits: [B,14,256,256]
```

---

## 4) Where the “attention modules” are

```
Encoder:  SE inside every InvertedResidual block
          (cheap channel attention many times)

Decoder:  CBAM-Lite once at /4 resolution
          (cheap spatial+channel refinement where boundaries exist)
```

---

## 5) Why skip-ADD (FPN) saves RAM vs U-Net skip-CONCAT

### U-Net skip concat (expensive)

```
up_features: [B, C, H, W]
skip       : [B, C, H, W]
concat ->    [B, 2C, H, W]   <-- channels double
then heavy convs on 2C
```

### LiteFieldSeg skip add (cheap)

```
up_features: [B, C, H, W]
skip_proj  : [B, C, H, W]   (via 1x1 conv)
add ->       [B, C, H, W]   <-- channels stay C
then cheap depthwise-sep conv
```

**This is one of the main memory + speed wins vs U-Net.**

---

# Bonus diagram: how this plugs into your field-aware training

Even though the model outputs pixel logits, you train with:

```
logits:   [B, C, H, W]
mask:     [B, H, W]        (0 = unlabeled/background)
field_id: [B, H, W]        (0 = not a field)
```

Field-aware loss conceptually:

```
For each field_id > 0:
    take pixels belonging to that field
    average logits over those pixels -> field_logit: [C]
    compute field_label from mask (majority vote excluding 0)
    CE(field_logit, field_label)

Total loss:
L = 0.2 * PixelLoss(ignore_index=0) + 1.0 * FieldLoss
```

So architecture stays “segmentation-like,” but training aligns to the field metric.
